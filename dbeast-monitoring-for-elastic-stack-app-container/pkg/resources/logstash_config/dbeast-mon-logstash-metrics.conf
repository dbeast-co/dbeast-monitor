input{
  http_poller {
    id => "Input-get-logstash-stats" 
    urls => {
      lab => "http://<LOGSTASH-API>/_node/stats"
    }
    request_timeout => 60
    schedule => { "cron" => "* * * * *"}
    codec => "json"
    add_field => { 
	  "[elasticsearch][cluster][id]" => "<CLUSTER_ID>"
	  "[event][module]" => "logstash"
	  "[event][dataset]" => "logstash_metrics"
	}
  }
}
filter{
  mutate {
    id => "MUTATE-remove-event-original"
    remove_field => ["[event][original]", "flow"]
  }
  
  clone {
    id => "CLONE-Duplicate-event-for-pipelines-event-processing"
    clones => ["pipelines"]
    add_field => {  
	  "[metricset][name]" => "pipelines" 
	}
  }
  
  if "pipelines" in [metricset][name]{
    ruby {
	  id => "RUBY-Move-pipelines-array-to-pipeline-object"
      code => '
	    pipelines = event.get("pipelines").to_hash;
        pipelienesArray = [];
        pipelines.each do |key, value|
          value["id"] = key;
          pipelienesArray << value;
        end
        event.remove("pipelines");
        event.set("pipelines_new", pipelienesArray);
	  '
    }
    mutate {
	  id => "MUTATE-remove-not-requeired-fields-from-pipeline-event"
      remove_field => ["jvm", "process", "events", "reloads", "os", "queue"]
    }
    split {
	  id => "SPLIT-Split-pipelines-to-separate-pipeline-to-dedicated-events"
      field => "pipelines_new"
      target => "[logstash][node][stats][pipelines]"
    }
	mutate {
	  id => "MUTATE-remove-old-pipeline-object"
      remove_field => ["pipelines_new"]
	  copy => { "[logstash][node][stats][pipelines][ephemeral_id]" => "[logstash][node][stats][pipelines][ephemeral_id_key]" }
	  copy => { "[logstash][node][stats][pipelines][id]" => "[logstash][log][pipeline_name]" }
    }
	aggregate {
	  id => "AGGREGATE-Calculate rates"
      task_id => "%{[logstash][node][stats][pipelines][ephemeral_id_key]}"
      code => "
	    require 'date';
		#Date in seconds
		sample_date = Time.now.to_i;
		event.set('sample_date', sample_date);
     
	 	#Calculate rates 
	 	if 	map.key?('last_sample_date')
	 	  date_diff = ( sample_date - map['last_sample_date'] )
		  #Total rates for pipeline
		  event.set('[logstash][node][stats][pipelines][events][filtered_rate]', map.key?('last_filtered') ? ((event.get('[logstash][node][stats][pipelines][events][filtered]').to_i - map['last_filtered']) / date_diff).ceil : 0);
		  event.set('[logstash][node][stats][pipelines][events][in_rate]', map.key?('last_in') ? ((event.get('[logstash][node][stats][pipelines][events][in]').to_i - map['last_in']) / date_diff).ceil : 0);
		  event.set('[logstash][node][stats][pipelines][events][out_rate]', map.key?('last_out') ? ((event.get('[logstash][node][stats][pipelines][events][out]').to_i - map['last_out']) / date_diff).ceil : 0);
		  event.set('[logstash][node][stats][pipelines][events][failures_rate]', map.key?('last_failures') ? ((event.get('[logstash][node][stats][pipelines][events][failures]').to_i - map['last_failures']) / date_diff).ceil : 0);
		  
		  
		  #Filter rates
		  filters = event.get('[logstash][node][stats][pipelines][plugins][filters]');
		  if !filters.nil? && !map['last_filter_plugins'].nil?
		    filters.each_with_index do |new_plugin_value, index|
              map['last_filter_plugins'].each do |old_plugin_value|
                if old_plugin_value['id'] == new_plugin_value['id']
                  filters[index]['events'].key?('in') ? filters[index]['events']['in_rate'] = ((filters[index]['events']['in'] - old_plugin_value['events']['in'])/date_diff).ceil : 0;
                  if filters[index].key?('failures') and filters[index]['failures'] > 0
				    filters[index]['failures_rate'] = ((filters[index]['failures'] - old_plugin_value['failures'])/date_diff).ceil;
                  else
				    filters[index]['failures_rate'] = 0;
					filters[index]['failures'] = 0;
				  end
                    filters[index]['events'].key?('out') ? filters[index]['events']['out_last_period'] = (filters[index]['events']['out'] - old_plugin_value['events']['out']) : 0;
                    filters[index]['events'].key?('out_last_period') ? filters[index]['events']['out_rate'] = ((filters[index]['events']['out_last_period'])/date_diff).ceil : 0;
                    ( filters[index]['events'].key?('duration_in_millis') and filters[index]['events']['out_last_period'] > 0) ? filters[index]['events']['latency'] = ((filters[index]['events']['duration_in_millis'] - old_plugin_value['events']['duration_in_millis'])/filters[index]['events']['out_last_period']) : 0;
				  end
              end
		    end
			event.set('[logstash][node][stats][pipelines][filters_failures]', filters.sum { |filter| filter['failures'] });
			event.set('[logstash][node][stats][pipelines][filters_failures_rate]', filters.sum { |filter| filter['failures_rate'] });
		  else
			event.set('[logstash][node][stats][pipelines][filters_failures]', 0 );		  
		  end	
          map['last_filter_plugins'] =  filters;
		  event.set('[logstash][node][stats][pipelines][plugins][filters]', filters);
     	  map['first_doc'] = false;
	 	else
		  map['last_filter_plugins'] =  event.get('[logstash][node][stats][pipelines][plugins][filters]');
	 	  map['first_doc'] = true;
	 	end
	 	map['last_sample_date'] =  sample_date;	  
		
	 	map['last_filtered'] =  event.get('[logstash][node][stats][pipelines][events][filtered]').to_i;	  
	 	map['last_in'] =  event.get('[logstash][node][stats][pipelines][events][in]').to_i;	  
	 	map['last_out'] =  event.get('[logstash][node][stats][pipelines][events][out]').to_i;	  
	 	map['last_failures'] =  event.get('[logstash][node][stats][pipelines][events][failures]').to_i;	  

	    #Drop document if there is no preview documents.
	 	if map['first_doc']
	 	  event.cancel();
	 	end
      "
      timeout => 3000000
      push_map_as_event_on_timeout => false
    }
	mutate {
	  id => "MUTATE-rename-pipeliens-field"
      rename => {
	    "[logstash][node][stats][pipelines]" => "[logstash][node][stats][pipeline]"
      }
    }
  }
  else {
    mutate {
      id => "MUTATE-transfor-stats-fields-to-ecs"
	  remove_field => ["pipelines"]
	  add_field => {  "[metricset][name]" => "node_stats" }
  	  rename => {
  	    "jvm" => "[logstash][node][stats][jvm]"
  	    "process" => "[logstash][node][stats][process]"
  	    "events" => "[logstash][node][stats][events]"
  	    "reloads" => "[logstash][node][stats][reloads]"
  	    "os" => "[logstash][node][stats][os]"
  	    "queue" => "[logstash][node][stats][queue]"
  	  }
    }
	aggregate {
	  id => "AGGREGATE-Calculate-rates"
      task_id => "%{ephemeral_id}"
      code => "
	     require 'date';
	 	#Date in seconds
	 	sample_date = Time.now.to_i;
	 	event.set('sample_date', sample_date);
     
	 	#Calculate rates 
	 	if 	map.key?('last_sample_date')
	 	  date_diff = ( sample_date - map['last_sample_date'] )
     
	 	  event.set('[logstash][node][stats][events][in_rate]', map.key?('last_events_in') ? ((event.get('[logstash][node][stats][events][in]').to_i - map['last_events_in']) / date_diff).ceil : 0);
		  event.set('[logstash][node][stats][events][filtered_rate]', map.key?('last_events_filtered') ? ((event.get('[logstash][node][stats][events][filtered]').to_i - map['last_events_filtered']) / date_diff).ceil : 0);
		  event.set('[logstash][node][stats][events][out_lats_period]', map.key?('last_events_out') ? (event.get('[logstash][node][stats][events][out]').to_i - map['last_events_out']) : 0);
		  event.set('[logstash][node][stats][events][out_rate]', map.key?('last_events_out') ? (event.get('[logstash][node][stats][events][out_lats_period]').to_i / date_diff).ceil : 0);
		  event.set('[logstash][node][stats][events][latency]', ( map.key?('last_events_duration_in_millis') and event.get('[logstash][node][stats][events][out_lats_period]') > 0) ? ((event.get('[logstash][node][stats][events][duration_in_millis]').to_i - map['last_events_duration_in_millis']) / event.get('[logstash][node][stats][events][out_lats_period]')) : 0);
	 	 
	 	  map['first_doc'] = false;
	 	else
	 	  map['first_doc'] = true;
	 	end
	 	
	 	map['last_sample_date'] =  sample_date;
     
	 	#Define the values for the next document
         map['last_events_in'] =  event.get('[logstash][node][stats][events][in]').to_i;
         map['last_events_filtered'] =  event.get('[logstash][node][stats][events][filtered]').to_i;
         map['last_events_out'] =  event.get('[logstash][node][stats][events][out]').to_i;
         map['last_events_duration_in_millis'] =  event.get('[logstash][node][stats][events][duration_in_millis]').to_i;
      	
	    #Drop document if there is no preview documents.
	 	if map['first_doc']
	 	  event.cancel();
	 	end
      "
      timeout => 3000000
      push_map_as_event_on_timeout => false
    }
  }
  mutate {
    id => "MUTATE-transform-additional-fields-to-ecs"
    rename => {
      "host" => "[logstash][node][stats][logstash][host]"
      "version" => "[logstash][node][stats][logstash][version]"
      "http_address" => "[logstash][node][stats][logstash][http_address]"
      "id" => "[logstash][node][stats][logstash][id]"
      "name" => "[logstash][node][stats][logstash][name]"
      "ephemeral_id" => "[logstash][node][stats][logstash][ephemeral_id]"
      "status" => "[logstash][node][stats][logstash][status]"
      "snapshot" => "[logstash][node][stats][logstash][snapshot]"
    }
	remove_field => ["monitoring", "events", "[logstash][node][stats][pipeline][plugins][outputs][flow]"]
	copy => {
	  "[elasticsearch][cluster][id]" => "[logstash][elasticsearch][cluster][id]"
	  "[logstash][node][stats][logstash][host]" => "[host][hostname]"
	}
  }
}
output{
  if [elasticsearch][cluster][id] {
    if "pipelines" in [metricset][name]{
      elasticsearch {
  	  id => "es-output-send-pipelines-to-mon-logstash-metrics-status-index"
  	  hosts => ["<MON_HOST>"]
  	  user => "<MON_USER>"
  	  password => "<MON_PASSWORD>"

  	  ssl_enabled => <MON_SSL_ENABLED>
  	  ssl_verification_mode => none
      index => "dbeast-mon-index-logstash-metrics-status"
      document_id => "%{[logstash][node][stats][logstash][id]}-%{[logstash][node][stats][pipeline][id]}"
  	  manage_template => false
      }
    }
    else {
      elasticsearch {
        id => "es-output-send-to-mon-logstash-metrics-status-index"
    	hosts => ["<MON_HOST>"]
    	user => "<MON_USER>"
    	password => "<MON_PASSWORD>"

    	ssl_enabled => <MON_SSL_ENABLED>
    	ssl_verification_mode => none
        index => "dbeast-mon-index-logstash-metrics-status"
        document_id => "%{[logstash][node][stats][logstash][id]}"
    	manage_template => false
      }
    }
    elasticsearch {
      id => "es-output-send-to-mon-logstash-metrics-historical-index"
  	  hosts => ["<MON_HOST>"]
  	  user => "<MON_USER>"
  	  password => "<MON_PASSWORD>"

  	  ssl_enabled => <MON_SSL_ENABLED>
  	  ssl_verification_mode => none
      index => "dbeast-mon-tsds-logstash-metrics"
      action => "create"
  	  manage_template => false
    }
  }
  else {
    elasticsearch {
      id => "es-output-send-to-corrupted-data"
  	  hosts => ["<MON_HOST>"]
  	  user => "<MON_USER>"
  	  password => "<MON_PASSWORD>"

  	  ssl_enabled => <MON_SSL_ENABLED>
  	  ssl_verification_mode => none
      index => "dbeast-mon-index-corrupted-data"
  	  manage_template => false
    }
  }
}
