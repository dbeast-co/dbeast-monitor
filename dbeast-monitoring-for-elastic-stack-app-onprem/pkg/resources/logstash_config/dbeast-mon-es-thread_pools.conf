input{
  http_poller {
    id => "INPUT-Get-thread-pools"
    urls => {
      lab => "<PROD_HOST>/_cat/thread_pool?format=json&h=*"
    }
	user => "<PROD_USER>"
	password => "<PROD_PASSWORD>"
    
    request_timeout => 60
    schedule => { "cron" => "* * * * *"}
    codec => "json"
    ssl_verification_mode => "none"    
    add_field => {
	  "[elasticsearch][cluster][id]" => "<CLUSTER_ID>"
	  "[event][module]" => "elasticsearch"
	  "[event][dataset]" => "thread_pool"
    }
  }  
}
filter{
##### Calculate rates  
  aggregate {
    id => "AGGREGATE-Generate-rates" 
     task_id => "%{[elasticsearch][cluster][id]}-%{node_id}-%{name}"
     code => "
	    require 'date';
		#Date in seconds
		sample_date = Time.now.to_i;
		event.set('timestamp', sample_date);

		#Calculate rates 
		if 	map.key?('last_sample_date')
		  date_diff = ( sample_date - map['last_sample_date'] )
		  event.set('rejected_rate', map.key?('last_rejected') ? ((event.get('rejected').to_i - map['last_rejected']) / date_diff).ceil : 0);
		  event.set('completed_rate', map.key?('last_completed') ? ((event.get('completed').to_i - map['last_completed']) / date_diff).ceil : 0);

		  map['first_doc'] = false;
		else
		  map['first_doc'] = true;
		end

		#Define the values for the next document
		map['last_sample_date'] = sample_date;
        map['last_rejected'] = event.get('rejected').to_i;
        map['last_completed'] = event.get('completed').to_i;

       #Drop document if there is no preview documents.
		if map['first_doc']
		  event.cancel();
		end
     "
     timeout => 3000000
     push_map_as_event_on_timeout => false
  }

  
  date {
    id => "Transfer-original-timestamp"
    match => [ "timestamp", "UNIX" ]
  }
  
  mutate {
    id => "MUTATE-adapt-to-ecs"
    rename => {
	  "node_name" => "[elasticsearch][node][name]"
	  "node_id" => "[elasticsearch][node][id]"
	  "ip" => "[elasticsearch][node][ip]"
	  "port" => "[elasticsearch][node][port]"
	  "pid" => "[elasticsearch][node][pid]"
	  "ephemeral_node_id" => "[elasticsearch][node][ephemeral_node_id]"
	}
    remove_field => [ "timestamp" ]
  }
  
#####Move fields to the elasticsearch.node.stats.thread_pool field  
  
  ruby {
    id => "RUBY-Move-all-fields-to-eslaticsearch-ecs-fields"
    code => '
      hash = event.to_hash
      hash.each do |k, value|
	    if !k.include? "elasticsearch" and !k.include? "@timestamp" and !k.include? "event"
          event.set("[elasticsearch][node][stats][thread_pool][#{k}]", value);
          event.remove("[#{k}]");
		end
      end
    '
  }

}
output{
  if [elasticsearch][cluster][id] {
    elasticsearch {
      id => "es-output-send-to-thread_pools-historical-index"
  	  hosts => ["<MON_HOST>"]
  	  user => "<MON_USER>"
  	  password => "<MON_PASSWORD>"

      ssl_enabled => <MON_SSL_ENABLED>
  	  ssl_verification_mode => none
      index => "dbeast-mon-tsds-es-thread_pools"
      action => "create"
  	  manage_template => false
    }
    elasticsearch {
      id => "es-output-send-to-thread_pools-status-index"
  	  hosts => ["<MON_HOST>"]
  	  user => "<MON_USER>"
  	  password => "<MON_PASSWORD>"

      ssl_enabled => <MON_SSL_ENABLED>
  	  ssl_verification_mode => none
      index => "dbeast-mon-index-es-thread_pools-status"
  	  document_id => "%{[elasticsearch][cluster][id]}-%{[elasticsearch][node][id]}-%{[elasticsearch][node][stats][thread_pool][name]}"
  	  manage_template => false
    }
  }
  else {
    elasticsearch {
      id => "es-output-send-to-corrupted-data"
  	  hosts => ["<MON_HOST>"]
  	  user => "<MON_USER>"
  	  password => "<MON_PASSWORD>"

  	  ssl_enabled => <MON_SSL_ENABLED>
  	  ssl_verification_mode => none
      index => "dbeast-mon-index-corrupted-data"
  	  manage_template => false
    }
  }

}
