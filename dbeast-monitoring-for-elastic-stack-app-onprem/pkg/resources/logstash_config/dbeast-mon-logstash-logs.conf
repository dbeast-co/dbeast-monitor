input {
  file {
    id => "INPUT-FILE-logstash-logs"
    path => "<PATH_TO_LOGS>/logstash-plain*.log"
	start_position => beginning
	codec => multiline {
      pattern => "^\[%{TIMESTAMP_ISO8601}\]"
      negate => true
      what => "previous"
    }
    add_field => { 
	  "[elasticsearch][cluster][id]" => "<CLUSTER_ID>"
	  "[event][module]" => "logstash"
	  "[event][dataset]" => "logstash_logs"
	}  
  }
}
filter {
  ### Generic logs parsing with additional pipeline actions like: start, stop, TCP/UDP listeners start e.t.c.
  grok {
    id => "GROK-source-log-parsing"
    match => {
      "message" => [ 
      "\[%{TIMESTAMP_ISO8601:[logstash][log][timestamp]}\]\[%{LOGSTASH_LOGLEVEL:[log][level]}\s?\]\[%{LOGSTASH_CLASS_MODULE:[logstash][log][module]}\s*\]\[%{DATA:[logstash][log][pipeline_name]}\]\[%{DATA:[logstash][log][plugin_id]}\] %{PIPELINE_ACTION:[logstash][log][pipeline_action]} \{%{GREEDYDATA:[logstash][message]}",
      "\[%{TIMESTAMP_ISO8601:[logstash][log][timestamp]}\]\[%{LOGSTASH_LOGLEVEL:[log][level]}\s?\]\[%{LOGSTASH_CLASS_MODULE:[logstash][log][module]}\s*\]\[%{DATA:[logstash][log][pipeline_name]}\]\[%{DATA:[logstash][log][plugin_id]}\] %{GREEDYMULTILINE:[logstash][message]}",
      "\[%{TIMESTAMP_ISO8601:[logstash][log][timestamp]}\]\[%{LOGSTASH_LOGLEVEL:[log][level]}\s*\]\[%{LOGSTASH_CLASS_MODULE:[logstash][log][module]}\s*\]\[%{DATA:[logstash][log][pipeline_name]}\] %{PIPELINE_ACTION:[logstash][log][pipeline_action]} \{%{GREEDYDATA:[logstash][message]}\}",
      "\[%{TIMESTAMP_ISO8601:[logstash][log][timestamp]}\]\[%{LOGSTASH_LOGLEVEL:[log][level]}\s*\]\[%{LOGSTASH_CLASS_MODULE:[logstash][log][module]}\s*\]\[%{DATA:[logstash][log][pipeline_name]}\] %{GREEDYMULTILINE:[logstash][message]}",
      "\[%{TIMESTAMP_ISO8601:[logstash][log][timestamp]}\]\[%{LOGSTASH_LOGLEVEL:[log][level]}\s*\]\[%{LOGSTASH_CLASS_MODULE:[logstash][log][module]}\s*\] %{GREEDYMULTILINE:[logstash][message]}",
      "\[%{TIMESTAMP_ISO8601:[logstash][log][timestamp]}\]\[%{LOGSTASH_LOGLEVEL:[log][level]}\s*\]\[%{LOGSTASH_CLASS_MODULE:[logstash][log][module]}\s*\] %{GREEDYDATA:[logstash][message]}",
      "\[%{TIMESTAMP_ISO8601:[logstash][log][timestamp]}\]%{GREEDYDATA:[logstash][message]}"
      ]
    }

    pattern_definitions => {
      "GREEDYMULTILINE" => "(.|\\n)*"
      "LOGSTASH_CLASS_MODULE" => "[\w\.]+"
      "LOGSTASH_LOGLEVEL" => "INFO|ERROR|DEBUG|FATAL|WARN|TRACE"
      "PIPELINE_ACTION" => "Pipeline started|Reloading pipeline|Pipeline terminated|Starting input listener|Starting tcp input listener|Starting syslog udp listener|Starting syslog tcp listener|Pipeline started|Pipeline terminated|Reloading pipeline"
    }
  }
  
  ### Parse input listeners pipelinees ports
  if "Starting input listener" in [logstash][log][pipeline_action] or "Starting tcp input listener" in [logstash][log][pipeline_action] 
    or "Starting syslog udp listener" in [logstash][log][pipeline_action] or "Starting syslog tcp listener" in [logstash][log][pipeline_action]{
    grok {
	  id => "GROK-extract-udp-tcp-listeners"
      match => {
        "[logstash][message]" => [
		":address=>\"%{GREEDYDATA:[logstash][log][pipeline_listener_host]}:%{DATA:[logstash][log][pipeline_listener_port]}\"\}",
		":address=>\"%{GREEDYDATA:[logstash][log][pipeline_listener_host]}:%{DATA:[logstash][log][pipeline_listener_port]}\"(.*)"
		]
      }
    }
    fingerprint {
	    id => "FINGERPRINT-build-doc-id-for-ports-status"
        source => [ "host", "[logstash][log][pipeline_listener_host]", "[logstash][log][pipeline_listener_port]"]
        target => "[logstash][log][pipeline_generated_id]"
        concatenate_sources => true
    }
    mutate { add_field => { "[logstash][log][type]" => "Pipeline listener" }}	
  }
  
  ### Catch exceptions
  elseif "Failed to execute action" in [logstash][message] { 
    mutate { 
	  id => "MUTATE-recognise-pipeline-config-exception"
      add_field => { 
        "[logstash][exception][type]" => "Pipeline configuration exception" 
        "[logstash][log][type]" => "Exception"
      }
    }
    grok {
      id => "GROK-extract-pipeline-name-for-pipeline-config-exception"
	  match => {
        "[logstash][message]" => [
		  "(.*)id=>:\"%{DATA:[logstash][log][pipeline_name]}\"",
		  "(.*)id=>:%{DATA:[logstash][log][pipeline_name]}, (.*)",
		  "(.*)id=>:%{DATA:[logstash][log][pipeline_name]} (.*)"
        ]
	  }
    }
  }
  elseif "Error parsing json" in [logstash][message] or "JSON parse error" in [logstash][message]{ 
    mutate { 
	  id => "MUTATE-recognise-json-parse-exception"
      add_field => { 
        "[logstash][exception][type]" => "JSON parsing error" 
        "[logstash][log][type]" => "Exception"
      }
    }
  }

  elseif "illegal_argument_exception" in [logstash][message] { 
    grok {
	  id => "GROK-recognise-illegal-argument-exception"
      match => {
       "[logstash][message]" => [
		  "(.*)\"reason\"=>\"(.*)failed to parse field \[%{DATA:[logstash][exception][mapping_exception_field]}\] (.*)\"reason\"=>\"%{GREEDYDATA:[logstash][exception][reason]}\"", 
		  "(.*)\"reason\"=>\"%{GREEDYDATA:[logstash][exception][reason]}\"",
		  "(.*)\"reason\":\"%{GREEDYDATA:[logstash][exception][reason]}\""
		]
      }
    }
	if "tried to parse field" in [logstash][exception][reason]{
	  grok {
	    id => "GROK-extract-mapping-exception-tag-for-illegal-argument-exception"
        match => {
          "[logstash][exception][reason]" => "(.*)tried to parse field\[%{DATA:[logstash][exception][mapping_exception_field]}\] (.*)"
          }
      }
	}
    mutate { 
	  id => "MUTATE-add-illegal-argument-exception-tag" 
      add_field => { 
        "[logstash][exception][type]" => "Illegal argument exception" 
        "[logstash][log][type]" => "Exception"
      }
    }
  }
  elseif "mapper_parsing_exception" in [logstash][message] { 
    grok {
	  id => "GROK-extract-mapping-exception-tag-for-mapper-parsing-exception"
      match => {
       "[logstash][message]" => [
		  "(.*)\"reason\"=>\"(.*)failed to parse field \[%{DATA:[logstash][exception][mapping_exception_field]}\] (.*)\"reason\"=>\"%{GREEDYDATA:[logstash][exception][reason]}\"", 
		  "(.*)\"reason\"=>\"%{GREEDYDATA:[logstash][exception][reason]}\""
		]
      }
    }
    mutate { 
	  id => "MUTATE-add-mapper-parsing-exception-tag" 
      add_field => { 
        "[logstash][exception][type]" => "Mapper exception" 
        "[logstash][log][type]" => "Exception"
      }
    }
  }
  elseif "Ruby exception" in [logstash][message] { 
    mutate { 
	  id => "MUTATE-add-ruby-exception-tag" 
      add_field => { 
        "[logstash][exception][type]" => "Ruby exception" 
        "[logstash][log][type]" => "Exception"
      }
    }
  }
  elseif "syslog tcp output exception" in [logstash][message] { 
    mutate { 
	  id => "MUTATE-add-tcp-output-exception-tag" 
      add_field => { 
        "[logstash][exception][type]" => "syslog tcp output exception" 
        "[logstash][log][type]" => "Exception"
      }
    }
    grok {
	  id => "GROK-extract-pipeline-name-and-port-for-tcp-output-exception"
      match => {
        "[logstash][message]" => [
		  "(.*)id=>:\"%{DATA:[logstash][log][pipeline_name]}\"",
		  "(.*):port=>%{DATA:[logstash][log][pipeline_listener_port]},"
		]
      }
    }
  }
  elseif "Failed to decode CEF payload" in [logstash][message] or "Failed to parse CEF" in [logstash][message] { 
    mutate { 
	  id => "MUTATE-add-cef-codec-exception-tag" 
      add_field => { 
        "[logstash][exception][type]" => "CEF codec exception" 
        "[logstash][log][type]" => "Exception"
      }
    }
  }
  elseif "DecoderException" in [logstash][message] { 
    mutate { 
	  id => "MUTATE-add-decoder-exception-tag" 
      add_field => { 
        "[logstash][exception][type]" => "Decoder exception" 
        "[logstash][log][type]" => "Exception"
      }
    }
  }
  elseif "Exception while parsing KV" in [logstash][message] { 
    grok {
	  id => "GROK-extract-reason-for-kv-parsing-exception"
      match => {
        "[logstash][message]" => [
		  "Exception while parsing KV \{%{GREEDYDATA:[logstash][exception][reason]}\}", 
		  "(.*)\"reason\"=>\"%{GREEDYDATA:[logstash][exception][reason]}\""
		]
      }
    }
    mutate { 
	  id => "MUTATE-add-kv-exception-tag" 
      add_field => { 
        "[logstash][exception][type]" => "KV plugin exception" 
        "[logstash][log][type]" => "Exception"
      }
    }
  }  
  elseif "Pipeline worker error" in [logstash][message] { 
    grok {
	  id => "GROK-extract-reason-for-pipeline-worker-exception"
      match => {
       "[logstash][message]" => [
		  "Pipeline worker error, the pipeline will be stopped \{%{GREEDYDATA:[logstash][exception][reason]}\}"
		]
      }
    }
    mutate { 
	  id => "MUTATE-add-pipeline-worker-exception-tag" 
      add_field => { 
        "[logstash][exception][type]" => "Pipeline worker error" 
        "[logstash][log][type]" => "Exception"
      }
    }
  }  
  else {
    mutate { 
	  id => "MUTATE-add-general-tag" 
	  add_field => { "[logstash][log][type]" => "General" }
	}
  }
  
  mutate { 
	id => "MUTATE-rename-ev-original-remove-path" 
	remove_field => ["path", "message"]
	add_field => {
	  "[logstash][node][stats][pipeline][id]" => "%{[logstash][log][pipeline_name]}"
	}
	copy => {
	  "[elasticsearch][cluster][id]" => "[logstash][elasticsearch][cluster][id]"
	}
  }
  if ![host][hostname] {
    if [host][name]{
      mutate {
        id => "MUTATE-Rename-host-name-if-hostname-not-exists"
         rename => {
          "[host][name]" => "[host][hostname]"
         }
      }
    }
    elseif [host] {
      mutate {
        id => "MUTATE-Rename-host-if-not-exists"
         rename => {
          "[host]" => "[host][hostname]"
         }
      }
    }
  }
}
output {
  elasticsearch{
  	id => "es-output-send-to-mon-logstash-logs-historical-index" 
	hosts => ["<MON_HOST>"]
	user => "<MON_USER>"
	password => "<MON_PASSWORD>"
	
    ssl_enabled => <MON_SSL_ENABLED>
	ssl_verification_mode => none
    index => "dbeast-mon-ds-logstash-logs"
    action => "create"
	manage_template => false
 }
 
 ### Index the Logstash pipeline input listeners ports for pipelines monitoring
 if [logstash][log][pipeline_generated_id] {
   elasticsearch{
  	 id => "es-output-send-to-mon-logstash-logs-pipelines-ports-status-index" 
	hosts => ["<MON_HOST>"]
	 user => "<MON_USER>"
	 password => "<MON_PASSWORD>"
	
     ssl_enabled => <MON_SSL_ENABLED>
	 ssl_verification_mode => none
     index => "dbeast-mon-index-logstash-logs-pipelines-ports-status"
     document_id => "%{[logstash][log][pipeline_generated_id]}"
	 manage_template => false
   }
 }
}
