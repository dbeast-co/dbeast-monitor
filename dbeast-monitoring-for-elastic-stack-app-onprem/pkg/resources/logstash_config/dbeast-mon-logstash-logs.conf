input {
  file {
    id => "INPUT-FILE-logstash-logs"
    path => "<PATH_TO_LOGS>/logstash-plain*.log"
	start_position => beginning
	codec => multiline {
      pattern => "^\[%{TIMESTAMP_ISO8601}\]"
      negate => true
      what => "previous"
    }
    add_field => {
	  "[elasticsearch][cluster][id]" => "<CLUSTER_ID>"
	  "[event][module]" => "logstash"
	  "[event][dataset]" => "logstash_logs"
	}
  }
}
filter {
  ### Generic logs parsing with additional pipeline actions like: start, stop, TCP/UDP listeners start e.t.c.
  grok {
    id => "GROK-source-log-parsing"
    match => {
      "message" => [
      "\[%{TIMESTAMP_ISO8601:[logstash][log][timestamp]}\]\[%{LOGSTASH_LOGLEVEL:[log][level]}\s?\]\[%{LOGSTASH_CLASS_MODULE:[logstash][log][module]}\s*\]\[%{DATA:[logstash][log][pipeline_name]}\]\[%{DATA:[logstash][log][plugin_id]}\] %{PIPELINE_ACTION:[logstash][log][pipeline_action]} \{%{GREEDYDATA:[logstash][message]}",
      "\[%{TIMESTAMP_ISO8601:[logstash][log][timestamp]}\]\[%{LOGSTASH_LOGLEVEL:[log][level]}\s?\]\[%{LOGSTASH_CLASS_MODULE:[logstash][log][module]}\s*\]\[%{DATA:[logstash][log][pipeline_name]}\]\[%{DATA:[logstash][log][plugin_id]}\] %{GREEDYMULTILINE:[logstash][message]}",
      "\[%{TIMESTAMP_ISO8601:[logstash][log][timestamp]}\]\[%{LOGSTASH_LOGLEVEL:[log][level]}\s*\]\[%{LOGSTASH_CLASS_MODULE:[logstash][log][module]}\s*\]\[%{DATA:[logstash][log][pipeline_name]}\] %{PIPELINE_ACTION:[logstash][log][pipeline_action]} \{%{GREEDYDATA:[logstash][message]}\}",
      "\[%{TIMESTAMP_ISO8601:[logstash][log][timestamp]}\]\[%{LOGSTASH_LOGLEVEL:[log][level]}\s*\]\[%{LOGSTASH_CLASS_MODULE:[logstash][log][module]}\s*\]\[%{DATA:[logstash][log][pipeline_name]}\] %{GREEDYMULTILINE:[logstash][message]}",
      "\[%{TIMESTAMP_ISO8601:[logstash][log][timestamp]}\]\[%{LOGSTASH_LOGLEVEL:[log][level]}\s*\]\[%{LOGSTASH_CLASS_MODULE:[logstash][log][module]}\s*\] %{GREEDYMULTILINE:[logstash][message]}",
      "\[%{TIMESTAMP_ISO8601:[logstash][log][timestamp]}\]\[%{LOGSTASH_LOGLEVEL:[log][level]}\s*\]\[%{LOGSTASH_CLASS_MODULE:[logstash][log][module]}\s*\] %{GREEDYDATA:[logstash][message]}",
      "\[%{TIMESTAMP_ISO8601:[logstash][log][timestamp]}\]%{GREEDYDATA:[logstash][message]}"
      ]
    }

    pattern_definitions => {
      "GREEDYMULTILINE" => "(.|\\n)*"
      "LOGSTASH_CLASS_MODULE" => "[\w\.]+"
      "LOGSTASH_LOGLEVEL" => "INFO|ERROR|DEBUG|FATAL|WARN|TRACE"
      "PIPELINE_ACTION" => "Pipeline started|Reloading pipeline|Pipeline terminated|Starting input listener|Starting tcp input listener|Starting syslog udp listener|Starting syslog tcp listener|Pipeline started|Pipeline terminated|Reloading pipeline"
    }

     remove_field => ["[event][original]"]
  }

  ### Parse input listeners pipelinees ports
  if "Starting input listener" in [logstash][log][pipeline_action] or "Starting tcp input listener" in [logstash][log][pipeline_action]
    or "Starting syslog udp listener" in [logstash][log][pipeline_action] or "Starting syslog tcp listener" in [logstash][log][pipeline_action]{
    grok {
	  id => "GROK-extract-udp-tcp-listeners"
      match => {
        "[logstash][message]" => [
		":address=>\"%{GREEDYDATA:[logstash][log][pipeline_listener_host]}:%{DATA:[logstash][log][pipeline_listener_port]}\"\}",
		":address=>\"%{GREEDYDATA:[logstash][log][pipeline_listener_host]}:%{DATA:[logstash][log][pipeline_listener_port]}\"(.*)"
		]
      }
    }
    fingerprint {
	    id => "FINGERPRINT-build-doc-id-for-ports-status"
        source => [ "host", "[logstash][log][pipeline_listener_host]", "[logstash][log][pipeline_listener_port]"]
        target => "[logstash][log][pipeline_generated_id]"
        concatenate_sources => true
    }
    mutate { add_field => { "[logstash][log][type]" => "Pipeline listener" }}
  }

  ### Catch exceptions
  elseif "Failed to execute action" in [logstash][message] {
    mutate {
	  id => "MUTATE-recognise-pipeline-config-exception"
      add_field => {
        "[logstash][exception][type]" => "Pipeline configuration exception"
        "[logstash][log][type]" => "Exception"
      }
    }
    grok {
      id => "GROK-extract-pipeline-name-for-pipeline-config-exception"
	  match => {
        "[logstash][message]" => [
          "(.*)pipeline_id:%{DATA:[logstash][log][pipeline_name]}, (.*)",
		  "(.*)id=>:\"%{DATA:[logstash][log][pipeline_name]}\"",
		  "(.*)id=>:%{DATA:[logstash][log][pipeline_name]}, (.*)",
		  "(.*)id=>:%{DATA:[logstash][log][pipeline_name]} (.*)"
        ]
	  }
    }
  }
  elseif "Error parsing json" in [logstash][message] or "JSON parse error" in [logstash][message]{
    mutate {
	  id => "MUTATE-recognise-json-parse-exception"
      add_field => {
        "[logstash][exception][type]" => "JSON parsing error"
        "[logstash][log][type]" => "Exception"
      }
    }
  }

  elseif "illegal_argument_exception" in [logstash][message] {
    grok {
	  id => "GROK-recognise-illegal-argument-exception"
      match => {
       "[logstash][message]" => [
		  "(.*)\"reason\"=>\"(.*)failed to parse field \[%{DATA:[logstash][exception][mapping_exception_field]}\] (.*)\"reason\"=>\"%{GREEDYDATA:[logstash][exception][reason]}\"",
		  "(.*)\"reason\"=>\"%{GREEDYDATA:[logstash][exception][reason]}\"",
		  "(.*)\"reason\":\"%{GREEDYDATA:[logstash][exception][reason]}\""
		]
      }
    }
	if "tried to parse field" in [logstash][exception][reason]{
	  grok {
	    id => "GROK-extract-mapping-exception-tag-for-illegal-argument-exception"
        match => {
          "[logstash][exception][reason]" => "(.*)tried to parse field\[%{DATA:[logstash][exception][mapping_exception_field]}\] (.*)"
          }
      }
	}
    mutate {
	  id => "MUTATE-add-illegal-argument-exception-tag"
      add_field => {
        "[logstash][exception][type]" => "Illegal argument exception"
        "[logstash][log][type]" => "Exception"
      }
    }
  }
  elseif "mapper_parsing_exception" in [logstash][message] {
    grok {
	  id => "GROK-extract-mapping-exception-tag-for-mapper-parsing-exception"
      match => {
       "[logstash][message]" => [
		  "(.*)\"reason\"=>\"(.*)failed to parse field \[%{DATA:[logstash][exception][mapping_exception_field]}\] (.*)\"reason\"=>\"%{GREEDYDATA:[logstash][exception][reason]}\"",
		  "(.*)\"reason\"=>\"%{GREEDYDATA:[logstash][exception][reason]}\""
		]
      }
    }
    mutate {
	  id => "MUTATE-add-mapper-parsing-exception-tag"
      add_field => {
        "[logstash][exception][type]" => "Mapper exception"
        "[logstash][log][type]" => "Exception"
      }
    }
  }
  elseif "Ruby exception" in [logstash][message] {
    mutate {
	  id => "MUTATE-add-ruby-exception-tag"
      add_field => {
        "[logstash][exception][type]" => "Ruby exception"
        "[logstash][log][type]" => "Exception"
      }
    }
  }
  elseif "syslog tcp output exception" in [logstash][message] {
    mutate {
	  id => "MUTATE-add-tcp-output-exception-tag"
      add_field => {
        "[logstash][exception][type]" => "syslog tcp output exception"
        "[logstash][log][type]" => "Exception"
      }
    }
    grok {
	  id => "GROK-extract-pipeline-name-and-port-for-tcp-output-exception"
      match => {
        "[logstash][message]" => [
		  "(.*)id=>:\"%{DATA:[logstash][log][pipeline_name]}\"",
		  "(.*):port=>%{DATA:[logstash][log][pipeline_listener_port]},"
		]
      }
    }
  }
  elseif "Failed to decode CEF payload" in [logstash][message] or "Failed to parse CEF" in [logstash][message] {
    mutate {
	  id => "MUTATE-add-cef-codec-exception-tag"
      add_field => {
        "[logstash][exception][type]" => "CEF codec exception"
        "[logstash][log][type]" => "Exception"
      }
    }
  }
  elseif "DecoderException" in [logstash][message] {
    mutate {
	  id => "MUTATE-add-decoder-exception-tag"
      add_field => {
        "[logstash][exception][type]" => "Decoder exception"
        "[logstash][log][type]" => "Exception"
      }
    }
  }
  elseif "Exception while parsing KV" in [logstash][message] {
    grok {
	  id => "GROK-extract-reason-for-kv-parsing-exception"
      match => {
        "[logstash][message]" => [
		  "Exception while parsing KV \{%{GREEDYDATA:[logstash][exception][reason]}\}",
		  "(.*)\"reason\"=>\"%{GREEDYDATA:[logstash][exception][reason]}\""
		]
      }
    }
    mutate {
	  id => "MUTATE-add-kv-exception-tag"
      add_field => {
        "[logstash][exception][type]" => "KV plugin exception"
        "[logstash][log][type]" => "Exception"
      }
    }
  }
  elseif "Pipeline worker error" in [logstash][message] {
    grok {
	  id => "GROK-extract-reason-for-pipeline-worker-exception"
      match => {
       "[logstash][message]" => [
		  "Pipeline worker error, the pipeline will be stopped \{%{GREEDYDATA:[logstash][exception][reason]}\}"
		]
      }
    }
    mutate {
	  id => "MUTATE-add-pipeline-worker-exception-tag"
      add_field => {
        "[logstash][exception][type]" => "Pipeline worker error"
        "[logstash][log][type]" => "Exception"
      }
    }
  }
  else {
    mutate {
	  id => "MUTATE-add-general-tag"
	  add_field => { "[logstash][log][type]" => "General" }
	}
  }

  if !([logstash][log][pipeline_name]) {
    mutate {
  	  id => "MUTATE-add-pipeline-name-if-not-exists"
  	  add_field => { "[logstash][log][pipeline_name]" => "General" }
  	}
  }

  mutate {
	id => "MUTATE-rename-ev-original-remove-path"
	remove_field => ["path", "message"]
	add_field => {
	  "[logstash][node][stats][pipeline][id]" => "%{[logstash][log][pipeline_name]}"
	}
	copy => {
	  "[elasticsearch][cluster][id]" => "[logstash][elasticsearch][cluster][id]"
	}
  }

  ### Fix host.hostname for the different Operation systems
  if ![host][hostname] {
    if [host][name]{
      mutate {
        id => "MUTATE-Rename-host-name-if-hostname-not-exists"
        rename => {
          "[host][name]" => "[host][hostname]"
        }
      }
    }
    elseif [host] {
      mutate {
        id => "MUTATE-Rename-host-if-not-exists"
        rename => {
          "[host]" => "[host][hostname]"
        }
      }
    }
  }
  if [agent][name] and [agent][name] not in [host][hostname] {
     mutate {
      id => "MUTATE-rename-hostname"
      rename => {
        "[agent][name]" => "[host][hostname]"
      }
    }
  }

  ### Aggregate exception events
  if "Exception" in [logstash][log][type] {
    if [logstash][exception][mapping_exception_field] {
      fingerprint {
        id => "FINGERPRINT-generate-exception-uid-with-mapping-field"
        source => [
          "[elasticsearch][cluster][id]",
          "[host][hostname]",
          "[logstash][log][pipeline_name]",
          "[logstash][exception][type]",
          "[logstash][exception][mapping_exception_field]"
        ]
        target => "[elasticsearch][exception][uid]"
        concatenate_sources => true
      }
    }
    else {
      fingerprint {
        id => "FINGERPRINT-generate-exception-uid"
        source => [
          "[elasticsearch][cluster][id]",
          "[host][hostname]",
          "[logstash][log][pipeline_name]",
          "[logstash][exception][type]"
        ]
        target => "[elasticsearch][exception][uid]"
        concatenate_sources => true
      }
    }
    aggregate {
      id => "AGGREGATE-aggregate-similar excetions"
      task_id => "%{[elasticsearch][exception][uid]}"
      code => "
        map['event_type'] = 'aggregation';
        map['aggregated_events_count'] ||=0;
        map['aggregated_events_count'] +=1;
        if map['aggregated_events_count'] == 1
          event.to_hash.each { |key, value | map[key] = value; }
        end
        event.cancel()
      "
      timeout => 60
      push_map_as_event_on_timeout => true
    }
    mutate {
      id => "MUTATE-rename-exceptions-count"
      rename => {
        "aggregated_events_count" => "[logstash][exception][count]"
      }
    }
  }
}
output {
  elasticsearch{
  	id => "es-output-send-to-mon-logstash-logs-historical-index"
	hosts => ["<MON_HOST>"]
	user => "<MON_USER>"
	password => "<MON_PASSWORD>"

    ssl_enabled => <MON_SSL_ENABLED>
	ssl_verification_mode => none
    index => "dbeast-mon-ds-logstash-logs"
    action => "create"
	manage_template => false
  }

  ### Index the Logstash pipeline input listeners ports for pipelines monitoring
  if [logstash][log][pipeline_generated_id] {
    elasticsearch{
  	  id => "es-output-send-to-mon-logstash-logs-pipelines-ports-status-index"
	  hosts => ["<MON_HOST>"]
	  user => "<MON_USER>"
	  password => "<MON_PASSWORD>"

      ssl_enabled => <MON_SSL_ENABLED>
	  ssl_verification_mode => none
      index => "dbeast-mon-index-logstash-logs-pipelines-ports-status"
      document_id => "%{[logstash][log][pipeline_generated_id]}"
	  manage_template => false
    }
  }
}
