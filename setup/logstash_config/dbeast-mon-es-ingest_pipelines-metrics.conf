input{
  http_poller {
    id => "Input-get-node-ingest-stats-from-ES"
    urls => {
      lab => "<PROD_HOST>/_nodes/ingest:true/stats?metric=ingest&filter_path=nodes.*.ingest,nodes.*.name"
    }
	user => "<PROD_USER>"
	password => "<PROD_PASSWORD>"
    
	request_timeout => 60
    schedule => { "cron" => "* * * * *"}
    codec => "json"

	ssl_verification_mode => "none"
    add_field => {
	  "[elasticsearch][cluster][id]" => "<CLUSTER_ID>"
	  "[event][module]" => "elasticsearch"
	  "[event][dataset]" => "pipeline_stats"
	}	
  }
}
filter{
    mutate {
      id => "MUTATE-Remove-event.original"
      remove_field => ["[event][original]"]
    }

    ruby {
      id => "RUBY-Nodes-to-array"
      code => '
        nodesArray = [];
    	if event.get("nodes")
    	  nodes = event.get("nodes").to_hash;
    	  nodes.each do |node_id, data|
    	    data["node_id"] = node_id;
            nodesArray << data;
    	    event.remove("[nodes]");
          end
        end
    	event.set("[elasticsearch][number_of_ingest_nodes]",nodesArray.length());
    	event.set("node", nodesArray[0]);
	  '
    }

    ruby {
      id => "RUBY-Transform-pipelines-to-array"
      code => '
	    node = event.get("node").to_hash;
        pipelinesArray = [];
        pipelines = node["ingest"]["pipelines"];
        pipelines.each do |key, value|
		  key.start_with?(".")
          value["id"] = key;
          pipelinesArray << value;
        end
		event.remove("[node][ingest][pipelines]");
        event.set("pipelines_new", pipelinesArray);
	  '
    }

    split {
      id => "SPLIT-split-pipelines-to-dedicated-documents"
      field => "pipelines_new"
      target => "[elasticsearch][node][stats][ingest_pipeline]"
    }

	mutate {
	  id => "MUTATE-rename-and-remove-fields"
	  rename => {
	    "[node][node_id]" => "[elasticsearch][node][id]"
		"[node][name]" => "[elasticsearch][node][name]"
	  }
      remove_field => ["pipelines_new"]
    }
	ruby {
      id => "RUBY-parse-processors"
	  code => '
	    processors = event.get("[elasticsearch][node][stats][ingest_pipeline][processors]");
		new_processors = []
		if !processors.nil?
		  processors.each_with_index do | processor, index |
		    processor.each do | processor_id, processor_value |
              processor_value["number_in_pipeline"] = index
              processor_value["id"] = processor_id
              new_processors << processor_value;
            end
		  end
		end
        event.set("[elasticsearch][node][stats][ingest_pipeline][processors]", new_processors);
	  '
    }
	aggregate {
	  id => "AGGREGATE-calculate-rates"
      task_id => "%{[elasticsearch][cluster][id]}%{[elasticsearch][node][stats][ingest_pipeline][id]}"
      code => "
	    require 'date';
		#Date in seconds
		sample_date = Time.now.to_i;
		event.set('sample_date', sample_date);

	 	#Calculate rates
	 	if 	map.key?('last_sample_date')
	 	  date_diff = ( sample_date - map['last_sample_date'] );
		  #Calculate rates for processor
		  if event.get('[elasticsearch][node][stats][ingest_pipeline][count]') > 0
		    number_of_ingest_nodes = event.get('[elasticsearch][number_of_ingest_nodes]')
		    event.set('[elasticsearch][node][stats][ingest_pipeline][last_failed]', (event.get('[elasticsearch][node][stats][ingest_pipeline][failed]') - map['last_pipeline_failed']) * number_of_ingest_nodes);
			event.set('[elasticsearch][node][stats][ingest_pipeline][failed_rate]', (event.get('[elasticsearch][node][stats][ingest_pipeline][last_failed]')/date_diff).ceil);
		    event.set('[elasticsearch][node][stats][ingest_pipeline][last_processed]', (event.get('[elasticsearch][node][stats][ingest_pipeline][count]') - map['last_pipeline_count']) * number_of_ingest_nodes);
			event.set('[elasticsearch][node][stats][ingest_pipeline][in_rate]', ((event.get('[elasticsearch][node][stats][ingest_pipeline][count]') - map['last_pipeline_count'])/date_diff).ceil);
		    processors = event.get('[elasticsearch][node][stats][ingest_pipeline][processors]');
		    if !processors.nil? && !map['last_processors'].nil?
		      processors.each_with_index do | processor, index|
				  processors[index]['stats']['last_processed'] = (processors[index]['stats']['count'].to_i - map['last_processors'][index]['stats']['count'].to_i) * number_of_ingest_nodes;
                  processors[index]['stats']['in_rate'] = processors[index]['stats']['last_processed'] > 0 ? ( processors[index]['stats']['last_processed'] /date_diff).ceil : 0;
				  processors[index]['stats']['last_failed'] = (processors[index]['stats']['failed'].to_i - map['last_processors'][index]['stats']['failed'].to_i) * number_of_ingest_nodes;
                  processors[index]['stats']['failed_rate'] = processors[index]['stats']['last_failed'] > 0 ? ( processors[index]['stats']['last_failed'] /date_diff).ceil : 0;
				  processors[index]['stats']['last_processing_time'] = (processors[index]['stats']['time_in_millis'].to_i - map['last_processors'][index]['stats']['time_in_millis'].to_i) * number_of_ingest_nodes;
                  processors[index]['stats']['latency'] = ( processors[index]['stats']['last_processed'] > 0 and processors[index]['stats']['last_processing_time'] > 0) ? ( processors[index]['stats']['last_processing_time'] / processors[index]['stats']['last_processed']).ceil : 0;
              end	
              event.set('[elasticsearch][node][stats][ingest_pipeline][processors]', processors);			 
              event.set('[elasticsearch][node][stats][ingest_pipeline][processors_failures]', processors.sum { |filter| filter['stats']['failed_rate'] }) 
		    end
		    map['last_processors'] =  processors;		
		  else
		    event.set('[elasticsearch][node][stats][ingest_pipeline][in_rate]', 0);
		  end
		  map['first_doc'] = false;
	 	else
	 	  map['first_doc'] = true;
		  map['last_processors'] =  event.get('[elasticsearch][node][stats][ingest_pipeline][processors]');
		end 
		
		map['last_pipeline_count'] = event.get('[elasticsearch][node][stats][ingest_pipeline][count]').to_i;
		map['last_pipeline_failed'] = event.get('[elasticsearch][node][stats][ingest_pipeline][failed]').to_i;

	 	map['last_sample_date'] =  sample_date;	  		
	    #Drop document if there is no preview documents.
	 	if map['first_doc']
	 	  event.cancel();
	 	end
      "
      timeout => 3000000
      push_map_as_event_on_timeout => false
    
  }	

  mutate {
    id => "MUTATE-remove-node-field"
	remove_field => ["node"]
  }
}
output{
  if [elasticsearch][cluster][id] {
    elasticsearch {
      id => "es-output-send-to-ingest-pipelines-index-historical-index"
  	  hosts => ["<MON_HOST>"]
  	  user => "<MON_USER>"
  	  password => "<MON_PASSWORD>"

  	  ssl_enabled => <MON_SSL_ENABLED>
  	  ssl_certificate_verification => false
      index => "dbeast-mon-tsds-es-ingest_pipelines"
  	  action => "create"
  	  manage_template => false
    }
    elasticsearch {
      id => "es-output-send-to-ingest-pipelines-index-status-index"
  	  hosts => ["<MON_HOST>"]
  	  user => "<MON_USER>"
  	  password => "<MON_PASSWORD>"

  	  ssl_enabled => <MON_SSL_ENABLED>
  	  ssl_certificate_verification => false
  	  index => "dbeast-mon-index-es-ingest_pipelines-status"
  	  manage_template => false
  	  document_id => "%{[elasticsearch][cluster][id]}%{[elasticsearch][node][name]}%{[elasticsearch][node][stats][ingest_pipeline][id]}"
    }
  }
  else {
    elasticsearch {
      id => "es-output-send-to-corrupted-data"
  	  hosts => ["<MON_HOST>"]
  	  user => "<MON_USER>"
  	  password => "<MON_PASSWORD>"

  	  ssl_enabled => <MON_SSL_ENABLED>
  	  ssl_verification_mode => none
      index => "dbeast-mon-index-corrupted-data"
  	  manage_template => false
    }
  }
}
